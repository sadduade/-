{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gfe5I1miYG3t"
      },
      "outputs": [],
      "source": [
        "# %pip install pandas transformers tokenizers datasets xformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ANvUJHIUjOf0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "099a5782-0b2c-44b9-ef07-90181f75e7a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==2.2.2\n",
            "  Using cached torch-2.2.2-cp310-cp310-manylinux1_x86_64.whl (755.5 MB)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.17.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.2.2) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.2) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.2.2) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.2.2) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.2) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.2.2) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.2) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.2) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.2) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.2) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.2) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.2) (12.1.0.106)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch==2.2.2)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.2) (12.1.105)\n",
            "Collecting triton==2.2.0 (from torch==2.2.2)\n",
            "  Using cached triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.2) (12.4.127)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.25.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.2.2) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.2.2) (1.3.0)\n",
            "Installing collected packages: triton, nvidia-nccl-cu12, torch\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 2.3.0\n",
            "    Uninstalling triton-2.3.0:\n",
            "      Successfully uninstalled triton-2.3.0\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.20.5\n",
            "    Uninstalling nvidia-nccl-cu12-2.20.5:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.20.5\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.3.0\n",
            "    Uninstalling torch-2.3.0:\n",
            "      Successfully uninstalled torch-2.3.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.2.1+cu121 requires torch==2.2.1, but you have torch 2.2.2 which is incompatible.\n",
            "torchtext 0.17.1 requires torch==2.2.1, but you have torch 2.2.2 which is incompatible.\n",
            "xformers 0.0.26.post1 requires torch==2.3.0, but you have torch 2.2.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-nccl-cu12-2.19.3 torch-2.2.2 triton-2.2.0\n",
            "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.40.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (0.19.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.19.0)\n",
            "Requirement already satisfied: xformers in /usr/local/lib/python3.10/dist-packages (0.0.26.post1)\n",
            "Requirement already satisfied: zstandard in /usr/local/lib/python3.10/dist-packages (0.22.0)\n",
            "Requirement already satisfied: jsonlines in /usr/local/lib/python3.10/dist-packages (4.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.14.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.31.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.2.2)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.29.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Collecting torch (from transformers[torch])\n",
            "  Using cached torch-2.3.0-cp310-cp310-manylinux1_x86_64.whl (779.1 MB)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.1.3)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.0.106)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch->transformers[torch])\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.105)\n",
            "Collecting triton==2.3.0 (from torch->transformers[torch])\n",
            "  Using cached triton-2.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168.1 MB)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->transformers[torch]) (12.4.127)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonlines) (23.2.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21.0->transformers[torch]) (5.9.5)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->transformers[torch]) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->transformers[torch]) (1.3.0)\n",
            "Installing collected packages: triton, nvidia-nccl-cu12, torch\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 2.2.0\n",
            "    Uninstalling triton-2.2.0:\n",
            "      Successfully uninstalled triton-2.2.0\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.19.3\n",
            "    Uninstalling nvidia-nccl-cu12-2.19.3:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.19.3\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.2.2\n",
            "    Uninstalling torch-2.2.2:\n",
            "      Successfully uninstalled torch-2.2.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.2.1+cu121 requires torch==2.2.1, but you have torch 2.3.0 which is incompatible.\n",
            "torchtext 0.17.1 requires torch==2.2.1, but you have torch 2.3.0 which is incompatible.\n",
            "torchvision 0.17.2 requires torch==2.2.2, but you have torch 2.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-nccl-cu12-2.20.5 torch-2.3.0 triton-2.3.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torch==2.2.2 torchvision\n",
        "!pip install transformers[torch] pandas tokenizers datasets xformers zstandard jsonlines"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import copy\n",
        "import logging\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Optional, Dict, Sequence\n",
        "import json\n",
        "import torch\n",
        "import transformers\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import Trainer"
      ],
      "metadata": {
        "id": "ixFM0jAmezKv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oJntnegqjEU7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "828ed8c6-7ae4-4ef2-de57-7ea28779d760"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/datasets/load.py:1486: FutureWarning: The repository for IlyaGusev/ru_turbo_alpaca contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/IlyaGusev/ru_turbo_alpaca\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "dataset = load_dataset(\"IlyaGusev/ru_turbo_alpaca\", split='train')\n",
        "print(type(dataset))\n",
        "print(dataset.column_names)\n",
        "print(type(dataset[0]))\n",
        "print(dataset[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JEjwiFc9m8hO"
      },
      "outputs": [],
      "source": [
        "IGNORE_INDEX = -100\n",
        "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
        "DEFAULT_EOS_TOKEN = \"</s>\"\n",
        "DEFAULT_BOS_TOKEN = \"</s>\"\n",
        "DEFAULT_UNK_TOKEN = \"</s>\"\n",
        "PROMPT_DICT = {\n",
        "    \"prompt_input\": (\n",
        "        \"Ниже будут инструкции для решения задачи, а также текст, содержащий больше контекста. \"\n",
        "        \"Напиши отвечающий запросу ответ.\\n\\n\"\n",
        "        \"### Инструкции:\\n{instruction}\\n\\n### Контекст:\\n{input}\\n\\n### Ответ:\"\n",
        "    ),\n",
        "    \"prompt_no_input\": (\n",
        "        \"Ниже будут инструкции для решения задачи. \"\n",
        "        \"Напиши отвечающий запросу ответ.\\n\\n\"\n",
        "        \"### Инструкции:\\n{instruction}\\n\\n### Ответ:\"\n",
        "    ),\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FFfOhPw3m8sY"
      },
      "outputs": [],
      "source": [
        "def _tokenize_fn(strings: Sequence[str], tokenizer: transformers.PreTrainedTokenizer) -> Dict:\n",
        "    \"\"\"Tokenize a list of strings.\"\"\"\n",
        "    tokenized_list = [\n",
        "        tokenizer(\n",
        "            text,\n",
        "            return_tensors=\"pt\",\n",
        "            max_length=tokenizer.model_max_length,\n",
        "            truncation=True,\n",
        "        )\n",
        "        for text in strings\n",
        "    ]\n",
        "    input_ids = labels = [tokenized.input_ids[0] for tokenized in tokenized_list]\n",
        "    input_ids_lens = labels_lens = [\n",
        "        tokenized.input_ids.ne(tokenizer.pad_token_id).sum().item() for tokenized in tokenized_list\n",
        "    ]\n",
        "    return dict(\n",
        "        input_ids=input_ids,\n",
        "        labels=labels,\n",
        "        input_ids_lens=input_ids_lens,\n",
        "        labels_lens=labels_lens,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6EaAZGeGrgvp"
      },
      "outputs": [],
      "source": [
        "def preprocess(\n",
        "    sources: Sequence[str],\n",
        "    targets: Sequence[str],\n",
        "    tokenizer: transformers.PreTrainedTokenizer,\n",
        ") -> Dict:\n",
        "    \"\"\"Preprocess the data by tokenizing.\"\"\"\n",
        "    examples = [s + t for s, t in zip(sources, targets)]\n",
        "    examples_tokenized, sources_tokenized = [_tokenize_fn(strings, tokenizer) for strings in (examples, sources)]\n",
        "    input_ids = examples_tokenized[\"input_ids\"]\n",
        "    labels = copy.deepcopy(input_ids)\n",
        "    for label, source_len in zip(labels, sources_tokenized[\"input_ids_lens\"]):\n",
        "        label[:source_len] = IGNORE_INDEX\n",
        "    return dict(input_ids=input_ids, labels=labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p7fyUdTcs_jI"
      },
      "outputs": [],
      "source": [
        "class SupervisedDataset(Dataset):\n",
        "    \"\"\"Dataset for supervised fine-tuning.\"\"\"\n",
        "\n",
        "    def __init__(self, data_path: str, tokenizer: transformers.PreTrainedTokenizer):\n",
        "        super(SupervisedDataset, self).__init__()\n",
        "        logging.warning(\"Loading data...\")\n",
        "        list_data_dict = json.load(open(data_path))\n",
        "\n",
        "        logging.warning(\"Formatting inputs...\")\n",
        "        prompt_input, prompt_no_input = PROMPT_DICT[\"prompt_input\"], PROMPT_DICT[\"prompt_no_input\"]\n",
        "        sources = [\n",
        "            prompt_input.format_map(example) if example.get(\"input\", \"\") != \"\" else prompt_no_input.format_map(example)\n",
        "            for example in list_data_dict\n",
        "        ]\n",
        "        targets = [f\"{example['output']}{tokenizer.eos_token}\" for example in list_data_dict]\n",
        "\n",
        "        logging.warning(\"Tokenizing inputs... This may take some time...\")\n",
        "        data_dict = preprocess(sources, targets, tokenizer)\n",
        "\n",
        "        self.input_ids = data_dict[\"input_ids\"]\n",
        "        self.labels = data_dict[\"labels\"]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n",
        "        return dict(input_ids=self.input_ids[i], labels=self.labels[i])\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class DataCollatorForSupervisedDataset(object):\n",
        "    \"\"\"Collate examples for supervised fine-tuning.\"\"\"\n",
        "\n",
        "    tokenizer: transformers.PreTrainedTokenizer\n",
        "\n",
        "    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n",
        "        input_ids, labels = tuple([instance[key] for instance in instances] for key in (\"input_ids\", \"labels\"))\n",
        "        input_ids = torch.nn.utils.rnn.pad_sequence(\n",
        "            input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
        "        )\n",
        "        labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=IGNORE_INDEX)\n",
        "        return dict(\n",
        "            input_ids=input_ids,\n",
        "            labels=labels,\n",
        "            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('dataset.json', 'w', encoding='utf-8') as f:\n",
        "    f.write('[')\n",
        "    for i, example in enumerate(dataset):\n",
        "        json_example = json.dumps({\n",
        "            'instruction': example['instruction'],\n",
        "            'input': example['input'],\n",
        "            'output': example['output']\n",
        "        }, ensure_ascii=False, indent=4)\n",
        "        if i > 0:\n",
        "            f.write(',\\n')\n",
        "        f.write(json_example)\n",
        "    f.write(']')"
      ],
      "metadata": {
        "id": "3rz_TmDznFaG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l3Ip4mwRtEYI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2993fa2-cc22-4676-9b51-18f0db34b4b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "model_name = \"ai-forever/rugpt3small_based_on_gpt2\"\n",
        "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        max_length=512,\n",
        "        cache_dir=\"huggingface_cache\",\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qVxonJ2ZufoH"
      },
      "outputs": [],
      "source": [
        "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
        "    model_name,\n",
        "    cache_dir=\"huggingface_cache\",\n",
        "    model_max_length=512,\n",
        "    padding_side=\"right\",\n",
        "    use_fast=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YciwnnrDA6qH"
      },
      "outputs": [],
      "source": [
        "model_name = \"ai-forever/rugpt3small_based_on_gpt2\"\n",
        "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        max_length=512,\n",
        "        cache_dir=\"huggingface_cache\",\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sIemT-_GBCWF"
      },
      "outputs": [],
      "source": [
        "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
        "    model_name,\n",
        "    cache_dir=\"huggingface_cache\",\n",
        "    model_max_length=512,\n",
        "    padding_side=\"right\",\n",
        "    use_fast=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8eG0YStnBIYy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3142cf28-c713-4db5-ca32-9d43e94682e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Loading data...\n",
            "WARNING:root:Formatting inputs...\n",
            "WARNING:root:Tokenizing inputs... This may take some time...\n"
          ]
        }
      ],
      "source": [
        "train_dataset = SupervisedDataset(tokenizer=tokenizer, data_path='dataset.json')\n",
        "data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XvTJYl_GBL5W"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "train_args = TrainingArguments(\n",
        "    learning_rate=1e-5,\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=1,\n",
        "    evaluation_strategy='no',\n",
        "    weight_decay=0.,\n",
        "    warmup_ratio=0.03,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    save_strategy='no',\n",
        "    logging_steps=1000,\n",
        "    output_dir=\"rugpt_instruct_ft\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGfTkMhGBOzf"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(model=model,\n",
        "                 tokenizer=tokenizer,\n",
        "                 args=train_args,\n",
        "                 train_dataset=train_dataset,\n",
        "                 eval_dataset=None,\n",
        "                 data_collator=data_collator)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 549
        },
        "id": "e6VdTdx5k6zv",
        "outputId": "757d9252-e9e0-4a74-da44-dd8b55277f46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='14911' max='14911' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [14911/14911 31:38, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>2.553300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>2.233900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>2.183700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>2.170500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>2.142600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>2.087700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7000</td>\n",
              "      <td>2.101700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8000</td>\n",
              "      <td>2.088200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9000</td>\n",
              "      <td>2.070800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10000</td>\n",
              "      <td>2.062900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11000</td>\n",
              "      <td>2.060400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12000</td>\n",
              "      <td>2.058900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13000</td>\n",
              "      <td>2.041900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14000</td>\n",
              "      <td>2.050600</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=14911, training_loss=2.130407000154039, metrics={'train_runtime': 1899.8368, 'train_samples_per_second': 15.697, 'train_steps_per_second': 7.849, 'total_flos': 2103221219328000.0, 'train_loss': 2.130407000154039, 'epoch': 1.0})"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model('rugpt_instruct_ft')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cr1HcDXeolMf",
        "outputId": "ca046ffa-9ff0-48b7-d114-4cc82a739d28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'max_length': 512}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = 'rugpt_instruct_ft'"
      ],
      "metadata": {
        "id": "jCezPm-Qoncm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, model_max_length=512, max_length=512)\n",
        "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, max_length=512)"
      ],
      "metadata": {
        "id": "cRCwlnBjotqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_for_instruction(instruction, text, model):\n",
        "    text = text.replace('\\n', ' ')\n",
        "    prompt = (\"Ниже приведено описание задания, сопровождаемое входными данными, которые предоставляют дополнительный контекст. \"\n",
        "              \"Напиши ответ, который соответствует запросу.\\n\\n\"\n",
        "              f\"### Описание задания:\\n{instruction}\\n\\n### Входные данные:\\n{text}\\n\\n### Ответ:\")\n",
        "\n",
        "    inputs = tokenizer([prompt],\n",
        "                        return_tensors=\"pt\", padding=True)\n",
        "\n",
        "    output_sequences = model.generate(\n",
        "        # this parameters are also important but you can read about them in the docs and just try changing them\n",
        "        num_beams=1,\n",
        "#         temperature=0.4,\n",
        "#         max_length=100,\n",
        "        max_new_tokens=20,\n",
        "#         no_repeat_ngram_size=3,\n",
        "    #     repetition_penalty= 5.0,\n",
        "    #     length_penalty=0.01,\n",
        "    #     early_stopping=True,\n",
        "    #     do_sample=True,\n",
        "    #     top_k=30,\n",
        "    #     top_p=0.8,\n",
        "        early_stopping=True,\n",
        "    #     num_return_sequences=3,\n",
        "        num_return_sequences= 1,\n",
        "        input_ids=inputs[\"input_ids\"],\n",
        "        attention_mask=inputs[\"attention_mask\"],\n",
        "        do_sample=False,  # disable sampling to test if batching affects output\n",
        "    )\n",
        "    summaries = tokenizer.batch_decode(output_sequences[:,len(inputs[0]):], skip_special_tokens=True)\n",
        "    return summaries[0]"
      ],
      "metadata": {
        "id": "eXgXD8FMovv_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"\n",
        "Барсуки, зарывшиеся под железнодорожными путями, остановили движение поездов на севере и юге Нидерландов, что привело к длительным отменам по крайней мере на двух линиях.\n",
        "Во вторник днем все поезда были остановлены на оживленной линии между южными городами Ден Босх и Бокстел после того, как животные зарылись в дамбу, несущую рельсы. Национальная железнодорожная компания сообщила, что линия не будет работать как минимум неделю.\n",
        "Подкоп означает, что \"рельсы могут проседать, и тогда безопасность движения поездов больше не гарантируется\", - говорится в заявлении ProRail, компании, обслуживающей железнодорожную сеть Нидерландов.\n",
        "Ранее в этом месяце барсуки также зарылись под рельсы в районе северной деревни Молкверум в провинции Фрисландия, что вывело линию из эксплуатации до следующего месяца, пока рабочие будут добиваться разрешения на перемещение животных.\n",
        "Барсуки являются охраняемыми животными в Нидерландах, поэтому железнодорожники должны получить разрешение на их перемещение или нарушение их среды обитания, прежде чем начать ремонт.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "pfla3oO6pkdi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "instruction = \"Перескажи этот текст.\"\n",
        "predict_for_instruction(instruction, text, model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "zbjTwZrxpl5c",
        "outputId": "dc22c535-c74b-414c-f867-ebd796169c8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:535: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Барсуки, зарывшиеся под железнодорожными путями, остановили движение поездов на севере и юге Нидерландов,'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "instruction = \"О каком животном идет речь в тексте?\"\n",
        "predict_for_instruction(instruction, text, model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "p-XMQ45FwYx_",
        "outputId": "98f183c7-ad48-4b64-ea5d-61e217a61443"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Барсуки - это млекопитающее, которое живет в Северной Африке, где они обитают в дикой природе'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text2 = \"\"\"\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "jz5iEKHvwleP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "instruction = \"Сколько дней в году?\"\n",
        "predict_for_instruction(instruction, text2, model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "MnXhq6QJwoGf",
        "outputId": "af455b0e-e586-4211-c5f7-66ac99cc1b32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:535: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'В год в среднем по России составляет около 10 дней.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "instruction = \"Придумай описание для книги о кулинарии\"\n",
        "predict_for_instruction(instruction, text2, model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "tJU7flpYxW4C",
        "outputId": "bcf5d2d5-4208-48e5-a13c-7d56e857d28e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Книга \"О вкусной и здоровой пище\" - это увлекательное и захватывающее путешествие в мир кулина'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text3 = \"\"\"Хороша ложка\"\"\""
      ],
      "metadata": {
        "id": "LZIieyKNxfEN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "instruction = \"Допиши\"\n",
        "predict_for_instruction(instruction, text3, model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "OST7HrTbxoB4",
        "outputId": "4d3bbad5-d2f8-4d88-a353-53697d49c4de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Хороша ложка,\\nОна полна\\nИ вкусна.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}