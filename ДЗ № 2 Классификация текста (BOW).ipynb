{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19312801",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import razdel\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from razdel import tokenize as razdel_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1fcbba91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.metrics.pairwise import cosine_distances, cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819e10e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Задание 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d33b65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('labeled.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bbc464e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "toxic\n",
       "0.0    0.66514\n",
       "1.0    0.33486\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.toxic.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "322c5140",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(data, stratify=data.toxic, test_size=0.1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6baa9112",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.reset_index(inplace=True)\n",
    "test.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ecb231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Векторизация с дефолтной токенизацией"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "37645407",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=10, max_df=0.3)\n",
    "X = vectorizer.fit_transform(train.comment) \n",
    "X_test = vectorizer.transform(test.comment) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "56531683",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train.toxic.values\n",
    "y_test = test.toxic.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "74206884",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=100, class_weight='balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "546f80a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(class_weight=&#x27;balanced&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(class_weight=&#x27;balanced&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(class_weight='balanced')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a6580518",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1.])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "09a50f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0179316c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.80      0.89      0.85      2847\n",
      "         1.0       0.74      0.57      0.65      1477\n",
      "\n",
      "    accuracy                           0.78      4324\n",
      "   macro avg       0.77      0.73      0.75      4324\n",
      "weighted avg       0.78      0.78      0.78      4324\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, preds, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d490b471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Векторизация с токенизацией razdel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "31809458",
   "metadata": {},
   "outputs": [],
   "source": [
    "from razdel import tokenize as razdel_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b2e25e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def razdel_tokenizer(text):\n",
    "    tokens = list(razdel_tokenize(text))\n",
    "    return ' '.join([token.text.lower() for token in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "19307c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(tokenizer=razdel_tokenizer, min_df=10, max_df=0.3)\n",
    "X = vectorizer.fit_transform(train.comment) \n",
    "X_test = vectorizer.transform(test.comment) \n",
    "y = train.toxic.values\n",
    "y_test = test.toxic.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2146ed4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.52      0.63      2847\n",
      "         1.0       0.45      0.76      0.57      1477\n",
      "\n",
      "    accuracy                           0.60      4324\n",
      "   macro avg       0.63      0.64      0.60      4324\n",
      "weighted avg       0.69      0.60      0.61      4324\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators=100, class_weight='balanced')\n",
    "clf.fit(X, y)\n",
    "preds = clf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, preds, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6090da25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Задание 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "26bba5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "56ee13cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Пользователь\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# COUNT_VEC и LogReg\n",
    "\n",
    "stop = list(stopwords.words('russian'))\n",
    "\n",
    "count_vec = CountVectorizer(lowercase=True, encoding='UTF-8', min_df=1, tokenizer=lambda text: [token.text for token in tokenize(text)], stop_words=stop)\n",
    "\n",
    "X = count_vec.fit_transform(train.comment)\n",
    "X_test = count_vec.transform(test.comment)\n",
    "\n",
    "y = train.toxic.values\n",
    "y_test = test.toxic.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2eaf3e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Пользователь\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(C=0.5, class_weight='balanced', max_iter=100, penalty='l1', solver='saga')\n",
    "clf.fit(X, y)\n",
    "clf.classes_\n",
    "preds = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "952df916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.86      0.77      0.81      2847\n",
      "         1.0       0.63      0.76      0.69      1477\n",
      "\n",
      "    accuracy                           0.77      4324\n",
      "   macro avg       0.75      0.76      0.75      4324\n",
      "weighted avg       0.78      0.77      0.77      4324\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, preds, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "904883f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[array(['(', ')', ',', '-', '.', '...', '?', 'po', 'азии', 'акт',\n",
       "         'баранов', 'бляядь', 'большей', 'большинство', 'боятся', 'бросил',\n",
       "         'бугуртят', 'будь', 'бутылку', 'быдло', 'важно', 'ваша', 'вашего',\n",
       "         'видите', 'виноват', 'воистину', 'вокруг', 'воспитывают', 'всей',\n",
       "         'вспомните', 'вся', 'всё', 'выжившего', 'выражаться', 'говна',\n",
       "         'говорить', 'говоришь', 'гопник', 'гопника', 'гопота', 'горах',\n",
       "         'города', 'городов', 'группу', 'давай', 'давали', 'даги', 'дагов',\n",
       "         'даю', 'действительно', 'детей', 'дикарями', 'друг', 'друга',\n",
       "         'другие', 'другим', 'друзья', 'думаете', 'думаешь', 'дырок',\n",
       "         'ебал', 'ебали', 'еблом', 'ещё', 'живёт', 'жопу', 'залили',\n",
       "         'замечать', 'заставили', 'захватить', 'защищая', 'знаешь',\n",
       "         'знакомые', 'зуб', 'иди', 'идёт', 'избивают', 'изменилось',\n",
       "         'империи', 'истории', 'историю', 'итоге', 'кавказ', 'кавказа',\n",
       "         'кавказцев', 'какого', 'класс', 'классе', 'кодеров', 'колени',\n",
       "         'колокольни', 'конечном', 'которое', 'которые', 'который',\n",
       "         'кочевниками', 'кулаками', 'лет', 'лица', 'лично', 'локальные',\n",
       "         'люблю', 'людей', 'массу', 'менее', 'митинг', 'мозгах', 'называют',\n",
       "         'написанное', 'например', 'народ-пидор', 'народов', 'настолько',\n",
       "         'нации', 'нациков', 'незнания', 'никто', 'нихуя', 'нищими', 'ножа',\n",
       "         'нормальную', 'обоссали', 'образ', 'обтекай', 'обычными', 'одного',\n",
       "         'ожидать', 'оккупации', 'оно', 'опасность', 'османов', 'особенно',\n",
       "         'очередной', 'ошибке', 'пальца', 'парня', 'пару', 'пидорашек',\n",
       "         'пидорашка', 'пидорашками', 'пидорашку', 'пиздец', 'пиздеце',\n",
       "         'планах', 'плохое', 'посадят', 'поставил', 'постоянно',\n",
       "         'построить', 'похож', 'почитай', 'превышение', 'пределов',\n",
       "         'представляет', 'приезжали', 'принципе', 'причины', 'пришла',\n",
       "         'проблема', 'проигрываю', 'просто', 'прочее', 'прочие', 'прошлом',\n",
       "         'рашке', 'ри', 'русачки', 'русачков', 'русню', 'русня', 'руснявая',\n",
       "         'русскими', 'русских', 'самообороны', 'свинью', 'сделает',\n",
       "         'сидели', 'сильнее', 'сколько', 'славщитом', 'словно', 'случайно',\n",
       "         'смог', 'смогли', 'снимают', 'совершенно', 'сорта', 'соседнем',\n",
       "         'сосёт', 'среднеазиатах', 'средней', 'среднем',\n",
       "         'среднестатистический', 'среднестатистического', 'ссут', 'ставит',\n",
       "         'стадо', 'столетия', 'стояли', 'стоят', 'существования', 'таких',\n",
       "         'таком', 'тебе', 'тобой', 'той', 'толпе', 'треде', 'тысячу',\n",
       "         'убьёте', 'унижаете', 'унижают', 'учился', 'факту', 'фразами',\n",
       "         'хороших', 'хотите', 'целом', 'целые', 'человека', 'чеченцы',\n",
       "         'чурки', 'школе', 'школу', 'это', 'являются'], dtype='<U67')],\n",
       " [array([',', '.', 'c', 'аноны', 'героизм', 'даун', 'е', 'ебать', 'жизнь',\n",
       "         'жопу', 'захотят', 'которая', 'кучу', 'ментам', 'минуту', 'никому',\n",
       "         'нужен', 'обенно', 'обычно', 'петушиный', 'пизда', 'поведением',\n",
       "         'подтирать', 'подумаешь', 'такие', 'таких', 'твоей', 'твой',\n",
       "         'тебе', 'ту', 'ть', 'ходом', 'ю'], dtype='<U67')],\n",
       " [array(['!', ',', '-', ':', '?', 'будь', 'вонючий', 'говно', 'дерьмо',\n",
       "         'жопа', 'иди', 'идиот', 'мать', 'негодяй', 'падла', 'поднять',\n",
       "         'попробуй', 'проклят', 'решил', 'семью', 'сука', 'сюда', 'твою',\n",
       "         'трахать', 'ублюдок', 'флаг', 'чертов'], dtype='<U67')],\n",
       " [array(['!', ',', '-', '...', '1', 'блять', 'васи', 'великий', 'вс', 'всё',\n",
       "         'добился', 'другие', 'еблан', 'жизни', 'заебали', 'какие',\n",
       "         'которым', 'меньше', 'начинают', 'обстоятельств', 'ой', 'попёрло',\n",
       "         'появились', 'прилично', 'сами', 'сделал', 'смотрите', 'ссука',\n",
       "         'стечению', 'тему', 'устроили', 'устроится', 'фанаты', 'хер',\n",
       "         'хуже'], dtype='<U67')],\n",
       " [array(['!', '!!!', ',', 'анкап', 'говно', 'говорите', 'государства',\n",
       "         'государство', 'ебут', 'живет', 'живут', 'жопе', 'контрактами',\n",
       "         'контракты', 'короче', 'лицо', 'любите', 'нациков', 'открыто',\n",
       "         'пацаны', 'пояснил', 'прямо', 'репутацию', 'рот', 'свободы',\n",
       "         'сегодня', 'смело', 'убрать', 'увидел', 'хардкор', 'шел'],\n",
       "        dtype='<U67')],\n",
       " [array(['!', ',', '.', 'америка', 'анальной', 'анальных', 'будьте', 'вами',\n",
       "         'вашего', 'вашу', 'верят', 'весь', 'видать', 'вновь',\n",
       "         'возвращается', 'время', 'встретить', 'вся', 'всякого', 'говне',\n",
       "         'граждане', 'дегенерат', 'детей', 'доводить', 'ебало', 'жопу',\n",
       "         'злобный', 'изнасиловать', 'континент', 'которого', 'лишь',\n",
       "         'насиловать', 'начался', 'никто', 'ночи', 'оперативные', 'отец',\n",
       "         'педераст', 'педофил', 'педофила', 'передоза', 'полиции',\n",
       "         'поступает', 'придти', 'самое', 'сатаны', 'сводки', 'сделать',\n",
       "         'северная', 'случай', 'собирается', 'стороны', 'суть', 'сына',\n",
       "         'такова', 'телефон', 'трясется', 'ужаса', 'форс', 'часа', 'это'],\n",
       "        dtype='<U67')],\n",
       " [array(['!', 'л', 'могут', 'нормально', 'сделать', 'строить', 'т',\n",
       "         'технологий', 'тупые'], dtype='<U67')],\n",
       " [array(['!', '(', ',', '-', '.', '00', '01', '2', '2019', '22', '23', '25',\n",
       "         '5', '6', ':', '?', '?!', 'b', 'ch', 'd', 'hk', 'm', 'абу',\n",
       "         'аватарка', 'аватарок', 'аниме', 'анон', 'анонов', 'ахуенно',\n",
       "         'бан', 'банит', 'бля', 'блядь', 'бред', 'вебм', 'вебмки', 'видео',\n",
       "         'видимо', 'вне', 'вообще', 'вопросы', 'ворох', 'вроде', 'второй',\n",
       "         'выпей', 'выписывают', 'глаза', 'говоря', 'давайте', 'дал',\n",
       "         'данных', 'даунов', 'двач', 'две', 'дебилов', 'деньги',\n",
       "         'должности', 'друг', 'другом', 'ебанутые', 'единственное', 'ещё',\n",
       "         'желанию', 'жертвой', 'жизни', 'жопу', 'забанили', 'застал',\n",
       "         'затем', 'злоупотребления', 'значит', 'идти', 'истекает', 'итоге',\n",
       "         'каких', 'какого', 'какое', 'капчу', 'которые', 'который',\n",
       "         'кстати', 'логика', 'могут', 'модератор', 'модератора',\n",
       "         'модерации', 'мочи', 'мочух', 'написав', 'немедленно', 'нему',\n",
       "         'никаких', 'никто', 'новом', 'обсуждение', 'обсуждения', 'общался',\n",
       "         'общее', 'одному', 'одну', 'остальных', 'ответов', 'отменить',\n",
       "         'отсюда', 'ошибка', 'пасскод', 'первое', 'пересмотреть',\n",
       "         'пиздюлей', 'писал', 'получаю', 'получить', 'пользователей',\n",
       "         'пообщатся', 'попал', 'попросил', 'постил', 'постинг', 'постинга',\n",
       "         'постить', 'посты', 'посчитал', 'потёрли', 'почему', 'пошёл',\n",
       "         'поэтому', 'предупреждение', 'причина', 'причинам', 'причине',\n",
       "         'продолжается', 'продолжать', 'просто', 'прошлого', 'разной',\n",
       "         'речь', 'своему', 'свои', 'связанные', 'связи', 'сделал', 'сказал',\n",
       "         'скинул', 'скинуть', 'скоростью', 'скрин', 'собирать', 'сообщение',\n",
       "         'сообщения', 'спасибо', 'справедливые', 'стал', 'стоит',\n",
       "         'сумасшедший', 'таблетки', 'твоего', 'текста', 'тематики', 'тему',\n",
       "         'тех', 'требую', 'тред', 'треда', 'тредах', 'треде', 'удаления',\n",
       "         'удаляет', 'улетел', 'херню', 'хрена', 'хуй', 'чатиков', 'че',\n",
       "         'человека', 'что-то', 'шебм', 'этим', 'это'], dtype='<U67')],\n",
       " [array(['!', ',', 'ваньки', 'венец', 'женской', 'женщина', 'женщины',\n",
       "         'какашки', 'кнн', 'колени', 'лошки', 'одной', 'победил', 'помните',\n",
       "         'правят', 'стоите', 'творения', 'феминизм', 'хуемрази',\n",
       "         'цивилизацией', 'это'], dtype='<U67')],\n",
       " [array([',', '.', '?', 'абу', 'блядь', 'вашего', 'ебал', 'ебаного',\n",
       "         'жопой', 'жопу', 'захожу', 'зашел', 'из-за', 'какого', 'клянусь',\n",
       "         'нахуй', 'послушать', 'рекомендует', 'сука', 'трубу', 'фильм',\n",
       "         'хрю', 'хуя', 'это', 'ютуб'], dtype='<U67')]]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percents_top10 = list(np.sort(clf.predict_proba(X_test)[:, 1])[::-1][:10])\n",
    "indices_top10  = list(np.argsort(clf.predict_proba(X_test)[:, 1])[::-1][:10])\n",
    "top10_sents = [count_vec.inverse_transform(X_test[i]) for i in indices_top10]\n",
    "top10_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c6e06fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Пользователь\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((10088, 55043), (4324, 55043))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TF-IDF_VEC и KNN\n",
    "vectorizer = TfidfVectorizer(lowercase=True, encoding='UTF-8', min_df=1, tokenizer=lambda text: [token.text for token in tokenize(text)], stop_words=stop)\n",
    "\n",
    "X = vectorizer.fit_transform(train.comment)\n",
    "X_test = vectorizer.transform(test.comment)\n",
    "\n",
    "y = train.toxic.values\n",
    "y_test = test.toxic.values\n",
    "\n",
    "X.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "49effd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = KNeighborsClassifier(n_neighbors=10)\n",
    "\n",
    "clf.fit(X, y)\n",
    "\n",
    "preds = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8c8403b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.77      0.96      0.85      2847\n",
      "         1.0       0.85      0.45      0.59      1477\n",
      "\n",
      "    accuracy                           0.79      4324\n",
      "   macro avg       0.81      0.71      0.72      4324\n",
      "weighted avg       0.80      0.79      0.76      4324\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, preds, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9cda9cb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[array(['!', ',', '.', '18', '?', 'авторитет', 'года', 'дело', 'держись',\n",
       "         'конце', 'кроме', 'мужик', 'станет', 'хохлоботов', 'этих'],\n",
       "        dtype='<U67')],\n",
       " [array([',', '.', '?', 'блядь', 'её', 'знают', 'отправить', 'парься',\n",
       "         'психушку', 'шизофреник'], dtype='<U67')],\n",
       " [array([',', '.', 'евреи', 'знаем', 'отобрать', 'придумали', 'русских'],\n",
       "        dtype='<U67')],\n",
       " [array([',', '.', 'абхазии', 'всё', 'выбили', 'выбить', 'говна', 'гум',\n",
       "         'дерьма', 'драмнбаса', 'забил', 'иглу', 'макак', 'никаких',\n",
       "         'получить', 'посадив', 'прибыль', 'проблема', 'профитов', 'путин',\n",
       "         'расеянин', 'сирии', 'уплатит', 'хуй'], dtype='<U67')],\n",
       " [array([',', 'каким', 'куколды', 'печь', 'пиздец', 'плачет', 'полном',\n",
       "         'просто', 'серьезе', 'сидят', 'сказки', 'слушают', 'чате', 'это',\n",
       "         'этому'], dtype='<U67')],\n",
       " [array(['....', 'ждал', 'кто-то', 'ой', 'хохлов', 'хуйни'], dtype='<U67')],\n",
       " [array([',', '.', 'вашу', 'понимаю', 'речь', 'счастью'], dtype='<U67')],\n",
       " [array([',', '.', 'её', 'знаешь', 'какие', 'метастазы', 'поразили'],\n",
       "        dtype='<U67')],\n",
       " [array([',', '.', '?', 'иди', 'мойся', 'обосрался', 'струю'], dtype='<U67')],\n",
       " [array([',', '.', 'ай', 'оу', 'ту', 'э'], dtype='<U67')]]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percents_top10 = list(np.sort(clf.predict_proba(X_test)[:, 1])[::-1][:10])\n",
    "indices_top10  = list(np.argsort(clf.predict_proba(X_test)[:, 1])[::-1][:10])\n",
    "top10_sents = [count_vec.inverse_transform(X_test[i]) for i in indices_top10]\n",
    "top10_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "797977b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Задание 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "cf3902d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['и',\n",
       " 'в',\n",
       " 'во',\n",
       " 'не',\n",
       " 'что',\n",
       " 'он',\n",
       " 'на',\n",
       " 'я',\n",
       " 'с',\n",
       " 'со',\n",
       " 'как',\n",
       " 'а',\n",
       " 'то',\n",
       " 'все',\n",
       " 'она',\n",
       " 'так',\n",
       " 'его',\n",
       " 'но',\n",
       " 'да',\n",
       " 'ты',\n",
       " 'к',\n",
       " 'у',\n",
       " 'же',\n",
       " 'вы',\n",
       " 'за',\n",
       " 'бы',\n",
       " 'по',\n",
       " 'только',\n",
       " 'ее',\n",
       " 'мне',\n",
       " 'было',\n",
       " 'вот',\n",
       " 'от',\n",
       " 'меня',\n",
       " 'еще',\n",
       " 'нет',\n",
       " 'о',\n",
       " 'из',\n",
       " 'ему',\n",
       " 'теперь',\n",
       " 'когда',\n",
       " 'даже',\n",
       " 'ну',\n",
       " 'вдруг',\n",
       " 'ли',\n",
       " 'если',\n",
       " 'уже',\n",
       " 'или',\n",
       " 'ни',\n",
       " 'быть',\n",
       " 'был',\n",
       " 'него',\n",
       " 'до',\n",
       " 'вас',\n",
       " 'нибудь',\n",
       " 'опять',\n",
       " 'уж',\n",
       " 'вам',\n",
       " 'ведь',\n",
       " 'там',\n",
       " 'потом',\n",
       " 'себя',\n",
       " 'ничего',\n",
       " 'ей',\n",
       " 'может',\n",
       " 'они',\n",
       " 'тут',\n",
       " 'где',\n",
       " 'есть',\n",
       " 'надо',\n",
       " 'ней',\n",
       " 'для',\n",
       " 'мы',\n",
       " 'тебя',\n",
       " 'их',\n",
       " 'чем',\n",
       " 'была',\n",
       " 'сам',\n",
       " 'чтоб',\n",
       " 'без',\n",
       " 'будто',\n",
       " 'чего',\n",
       " 'раз',\n",
       " 'тоже',\n",
       " 'себе',\n",
       " 'под',\n",
       " 'будет',\n",
       " 'ж',\n",
       " 'тогда',\n",
       " 'кто',\n",
       " 'этот',\n",
       " 'того',\n",
       " 'потому',\n",
       " 'этого',\n",
       " 'какой',\n",
       " 'совсем',\n",
       " 'ним',\n",
       " 'здесь',\n",
       " 'этом',\n",
       " 'один',\n",
       " 'почти',\n",
       " 'мой',\n",
       " 'тем',\n",
       " 'чтобы',\n",
       " 'нее',\n",
       " 'сейчас',\n",
       " 'были',\n",
       " 'куда',\n",
       " 'зачем',\n",
       " 'всех',\n",
       " 'никогда',\n",
       " 'можно',\n",
       " 'при',\n",
       " 'наконец',\n",
       " 'два',\n",
       " 'об',\n",
       " 'другой',\n",
       " 'хоть',\n",
       " 'после',\n",
       " 'над',\n",
       " 'больше',\n",
       " 'тот',\n",
       " 'через',\n",
       " 'эти',\n",
       " 'нас',\n",
       " 'про',\n",
       " 'всего',\n",
       " 'них',\n",
       " 'какая',\n",
       " 'много',\n",
       " 'разве',\n",
       " 'три',\n",
       " 'эту',\n",
       " 'моя',\n",
       " 'впрочем',\n",
       " 'хорошо',\n",
       " 'свою',\n",
       " 'этой',\n",
       " 'перед',\n",
       " 'иногда',\n",
       " 'лучше',\n",
       " 'чуть',\n",
       " 'том',\n",
       " 'нельзя',\n",
       " 'такой',\n",
       " 'им',\n",
       " 'более',\n",
       " 'всегда',\n",
       " 'конечно',\n",
       " 'всю',\n",
       " 'между']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = list(stopwords.words('russian'))\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "65985aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words=stop_words, max_features=9000, min_df=3, max_df=0.07)\n",
    "X = vectorizer.fit_transform(train.comment) \n",
    "X_test = vectorizer.transform(test.comment)\n",
    "y = train.toxic.values\n",
    "y_test = test.toxic.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e4fcb2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c77714e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.86      0.87      2847\n",
      "         1.0       0.75      0.78      0.76      1477\n",
      "\n",
      "    accuracy                           0.83      4324\n",
      "   macro avg       0.81      0.82      0.82      4324\n",
      "weighted avg       0.84      0.83      0.83      4324\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(C=0.1, class_weight='balanced')\n",
    "lr.fit(X, y)\n",
    "preds = lr.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "fc205306",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('тебе', 1.401223353403698),\n",
       " ('хохлы', 1.3522763030728544),\n",
       " ('хохлов', 1.346924136944774),\n",
       " ('нахуй', 1.0203327638376465),\n",
       " ('пиздец', 0.9841256547065218),\n",
       " ('хуй', 0.8880385198275667),\n",
       " ('блядь', 0.8855707259045223),\n",
       " ('блять', 0.8416125551199733),\n",
       " ('тред', 0.8279622981949827),\n",
       " ('сука', 0.7972458700368875)]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "coef = lr.coef_[0]\n",
    "\n",
    "feature_importance = dict(zip(feature_names, coef))\n",
    "\n",
    "top_toxic_words = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)\n",
    "top_toxic_words = [(word, coef) for word, coef in top_toxic_words][:10]\n",
    "top_toxic_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "400e53a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9d52f886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.76      0.79      2847\n",
      "         1.0       0.60      0.69      0.64      1477\n",
      "\n",
      "    accuracy                           0.74      4324\n",
      "   macro avg       0.71      0.72      0.72      4324\n",
      "weighted avg       0.75      0.74      0.74      4324\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dtc = DecisionTreeClassifier(max_depth=1200, class_weight='balanced')\n",
    "dtc.fit(X, y)\n",
    "preds = dtc.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "0bcbcb95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('тебе', 1.401223353403698),\n",
       " ('хохлы', 1.3522763030728544),\n",
       " ('хохлов', 1.346924136944774),\n",
       " ('нахуй', 1.0203327638376465),\n",
       " ('пиздец', 0.9841256547065218),\n",
       " ('хуй', 0.8880385198275667),\n",
       " ('блядь', 0.8855707259045223),\n",
       " ('блять', 0.8416125551199733),\n",
       " ('тред', 0.8279622981949827),\n",
       " ('сука', 0.7972458700368875)]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "feature_importance = dtc.feature_importances_\n",
    "\n",
    "top_toxic_words_dtc = [(feature_names[idx], importance) for idx, importance in enumerate(feature_importance)]\n",
    "top_toxic_words_dtc = sorted(top_toxic_words, key=lambda x: x[1], reverse=True)[:10]\n",
    "top_toxic_words_dtc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b1bf8184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "5f536c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      0.90      0.89      2847\n",
      "         1.0       0.80      0.74      0.77      1477\n",
      "\n",
      "    accuracy                           0.85      4324\n",
      "   macro avg       0.83      0.82      0.83      4324\n",
      "weighted avg       0.84      0.85      0.85      4324\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nb = MultinomialNB(alpha=1, fit_prior=False)\n",
    "nb.fit(X, y)\n",
    "preds = nb.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "999b5135",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(-6.014525313370823, 'просто'),\n",
       " (-6.059139361118227, 'тебе'),\n",
       " (-6.310675391696311, 'хохлы'),\n",
       " (-6.320719813838121, 'хохлов'),\n",
       " (-6.510448668137349, 'ещё'),\n",
       " (-6.526854354514809, 'нахуй'),\n",
       " (-6.540902732625231, 'почему'),\n",
       " (-6.603896385467091, 'хуй'),\n",
       " (-6.6146986072874006, 'пиздец'),\n",
       " (-6.669191315470519, 'вообще')]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "log_probs = nb.feature_log_prob_\n",
    "\n",
    "class_1_log_probs = log_probs[1] \n",
    "word_probs = list(zip(class_1_log_probs, feature_names))  \n",
    "top_toxic_words_nb = sorted(word_probs, reverse=True)[:10] \n",
    "top_toxic_words_nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f8c9382d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c3d8e4db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.63      0.73      2847\n",
      "         1.0       0.54      0.84      0.66      1477\n",
      "\n",
      "    accuracy                           0.70      4324\n",
      "   macro avg       0.71      0.73      0.70      4324\n",
      "weighted avg       0.77      0.70      0.71      4324\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=100, class_weight='balanced', max_depth=20)\n",
    "rf.fit(X, y)\n",
    "\n",
    "preds = rf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, preds, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "761d01ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('очень', 0.0228499138120917),\n",
       " ('хохлов', 0.019677561260564764),\n",
       " ('года', 0.019586775546079557),\n",
       " ('хохлы', 0.016771813770288517),\n",
       " ('например', 0.01568223587134489)]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "importances = rf.feature_importances_ \n",
    "\n",
    "top_indices = importances.argsort()[::-1][:5]\n",
    "\n",
    "top_toxic_words_rf = [(feature_names[idx], importances[idx]) for idx in top_indices]\n",
    "top_toxic_words_rf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
